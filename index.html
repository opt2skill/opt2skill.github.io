<!DOCTYPE html>
<html lang="en">
  <head>
      <meta charset="UTF-8">
      <link rel="stylesheet" href="main.css">
      <link rel="icon" type="image/x-icon" href="assets/digit.png">
      <title>Opt2Skill</title>
  </head>
  <body>


    <div id="title_slide">
        <div class="title_left">
            <h1>Opt2Skill: Imitating Dynamically-feasible Whole-Body <br> Trajectories for Versatile Humanoid Loco-Manipulation</h1>
        

            <div class="box alt">
              <div class="row uniform">
                <div class="2u"><a href="https://fukangl.github.io/">                   <span class="image fit"><img src="assets/author_photo/fukang.jpg" alt="Fukang Liu" /></span>Fukang Liu</a></div>
                <div class="2u"><a href="https://guzhaoyuan.com/">                      <span class="image fit"><img src="assets/author_photo/zhaoyuan.jpg" alt="Zhaoyuan Gu" /></span>Zhaoyuan Gu</a></div>
                <div class="2u"><a href="https://missinglight.github.io/">              <span class="image fit"><img src="assets/author_photo/yilin.jpg" alt="Yilin Cai" /></span>Yilin Cai</a></div>
                <div class="2u"><a href="https://ziyi-zhou.github.io/">                 <span class="image fit"><img src="assets/author_photo/ziyi.jpeg" alt="Ziyi Zhou" /></span>Ziyi Zhou</a></div>
                <div class="2u"><a href="https://hyunyoungjung.github.io/">             <span class="image fit"><img src="assets/author_photo/hyunyoung.jpg" alt="Hyunyoung Jung" /></span>Hyunyoung Jung</a></div>
                <div class="2u"><a href="https://opt2skill.github.io/">                 <span class="image fit"><img src="assets/author_photo/Jaehwi.png" alt="Jaehwi Jang" /></span>Jaehwi Jang</a></div>
                <div class="2u"><a href="https://opt2skill.github.io/">                 <span class="image fit"><img src="assets/author_photo/shijie.jpg" alt="Shijie Zhao" /></span>Shijie Zhao</a></div>
                <div class="2u"><a href="https://faculty.cc.gatech.edu/~sha9/">         <span class="image fit"><img src="assets/author_photo/sehoon.jpg" alt="Sehoon Ha" /></span>Sehoon Ha</a></div>
                <div class="2u"><a href="https://research.gatech.edu/people/yue-chen">  <span class="image fit"><img src="assets/author_photo/yue.jpg" alt="Yue Chen" /></span>Yue Chen</a></div>
                <div class="2u"><a href="https://faculty.cc.gatech.edu/~danfei/">       <span class="image fit"><img src="assets/author_photo/danfei.jpg" alt="Danfei Xu" /></span>Danfei Xu</a></div>
                <div class="2u"><a href="https://research.gatech.edu/people/ye-zhao">   <span class="image fit"><img src="assets/author_photo/ye.jpg" alt="Ye Zhao" /></span>Ye Zhao</a></div>
                  <!-- Add more authors in the same way -->
              </div>
          </div>

            <div class="gatech">
                <p>Georgia Institute of Technology</p>
            </div>
            <div class="acc">
            <p>RAL 2025</p>
            </div>
            <div class="button-container">
                <a href="https://arxiv.org/abs/2409.20514" class="button">Paper</a>
                <!-- <a href="https://www.youtube.com/watch?v=DRHfpCYXJfU" class="button">Video</a> -->
                 <a href="#" class="button disabled">Code</a>
            </div>

            <div class="code">
                <p>The code will be releasing soon.</p>
            </div>

            <br>
            <br>
            <br>
            <br>

            <div class="teaser">
                <div class="video_container">
                    <video autoplay muted playsinline loop controls preload="metadata">
                        <source src="assets/hardware/demo.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            
            <br>
            <div id="abstract" class="grid-container">
                <p>
                Humanoid robots are designed to perform diverse loco-manipulation tasks. However, they face challenges due to their high-dimensional and unstable dynamics, as well as the complex contact-rich nature of the tasks. 
                Model-based optimal control methods offer flexibility to define precise motion but are limited by high computational complexity and accurate contact sensing. 
                On the other hand, reinforcement learning (RL) handles high-dimensional spaces with strong robustness but suffers from inefficient learning, unnatural motion, and sim-to-real gaps. 
                To address these challenges, we introduce Opt2Skill, an end-to-end pipeline that combines model-based trajectory optimization with RL to achieve robust whole-body loco-manipulation. 
                Opt2Skill generates dynamic feasible and contact-consistent reference motions for the Digit humanoid robot using differential dynamic programming (DDP) and trains RL policies to track these optimal trajectories. 
                Our results demonstrate that Opt2Skill outperforms baselines that rely on human demonstrations and inverse kinematics-based references, both in motion tracking and task success rates. Furthermore, we show that incorporating trajectories with torque information improves contact force tracking in contact-involved tasks, such as wiping a table. 
                </p>
            </div>
        </div>
    </div>
    <hr class="rounded">
    
    <div id="overview">

        <h1>Diverse Walking Modes</h1>

        <p>
          Opt2Skill enables the Digit humanoid robot to perform diverse walking modes, including forward, backward, sideways, and turning gaits. 
        </p>


        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/forward_walking.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/backward_walking.mp4" type="video/mp4">
            </video>
        </div>


        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/side_walking_right.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/turn_walking.mp4" type="video/mp4">
            </video>
        </div>

        <p>
          In addition, it can handle rough terrain, such as stairs, ramps, and outdoor surfaces like grass.
        </p>
        


        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/stair_walking.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/ramp_walking.mp4" type="video/mp4">
            </video>
        </div>


        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/outdoor_walking_1.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/outdoor_walking_2.mp4" type="video/mp4">
            </video>
        </div>

        <p>
        We also observed that Opt2Skill may <strong>generalize beyond the training range</strong> in locomotion tasks. 
        For example, the training foot clearance ranged from 0.1 to 0.2 m, 
        but at test time, the policy successfully executed trajectories with 0.25 m clearance.
        </p>


        <h1>Diverse Contact-Rich Loco-Manipulation Tasks</h1>

        <p>
          Opt2Skill also enables the Digit humanoid robot to perform <strong>multi-contact whole-body loco-manipulation tasks</strong>, 
          such as pushing an object on a desk beyond its support polygon. 
          To maintain balance, the robot stabilizes itself by leaning on the table with one elbow while reaching out with the opposite arm to push the box. 
          The legs coordinate with upper-body motion to support this forward-reaching behavior, 
          demonstrating the need for <strong>whole-body coordination</strong>. 
          This example showcases Opt2Skill’s capability to handle high-dimensional contact-rich loco-manipulation tasks.
        </p>

        <div class="allegrofail">
            <div class="video_container">
                <video autoplay muted playsinline loop controls preload="metadata">
                    <source src="assets/hardware/new/desk_object_manipulation.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        
        
        
        <p>
          In shelf manipulation scenarios, the Digit robot squats to pick up a plastic box from a lower shelf and places it on a higher one. 
          The training hand target heights ranged from 0.55 to 1.70 m, 
          while the test scenario included targets at 0.4 m and 1.8 m in the z direction, 
          demonstrating that Opt2Skill can handle moderate variations <strong>beyond the training range</strong>.
        </p>  
        <p>
          Moreover, Opt2Skill achieves accurate tracking of hand positions throughout these complex motion sequences, 
          with an average tracking error of less than 4 cm <strong>without online trajectory adaptation</strong>.
        </p>


        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/shelf_object_manipulation_1.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/shelf_object_manipulation_2.mp4" type="video/mp4">
            </video>
        </div>


        <br>
        <p>
          Additionally, Opt2Skill enables other real-world tasks, 
          such as <i>heavy box pickup</i>, <i>pickup+walk</i>, <i>door opening</i>, and <i>drawing+wiping</i>. 

          <br>
        <p>
          In the <i>heavy box pickup</i> task, the robot squats down and successfully lifts a box weighing up to <strong>4.9 kg</strong>. 
          This task requires precise <strong>hand positioning</strong> and <strong>force control</strong> to lift the box without losing balance or dropping it.
        </p>

        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/box_pickup_0.3kg.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/box_pickup_4.9kg.mp4" type="video/mp4">
            </video>
        </div>

          <br>
          <p>
          In the <i>pickup+walk</i> task, the robot picks up a box from a standing pose and walks forward while carrying it. 
          This requires the robot to maintain balance and apply <strong>sufficient contact force</strong> to hold the box, 
          demanding accurate hand positioning and force control. 
          The task demonstrates Opt2Skill’s ability to adapt to <strong>dynamic load changes</strong> and maintain stability during locomotion.
          </p>


          <div class="allegrofail">
            <div class="video_container">
                <video autoplay muted playsinline loop controls preload="metadata">
                    <source src="assets/hardware/new/pick_walk.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        
        <br>
        <p>
          In the <i>door opening</i> task, the robot approaches two heavy fire doors and pushes them open using both arms while walking.
          This requires maintaining balance and applying <strong>sufficient force</strong>, 
          demonstrating the robot's ability to handle heavy objects and adapt to <strong>dynamic environments</strong>.
        </p>

        <div class="allegrofail">
            <div class="video_container">
                <video autoplay muted playsinline loop controls preload="metadata">
                    <source src="assets/hardware/new/singgle_door_opening.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/double_door_opening_1.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/double_door_opening_2.mp4" type="video/mp4">
            </video>
        </div>
          

        <br>
        <p>
          In the <i>drawing+wiping</i> task, the robot uses one hand to draw lines on a whiteboard and the other hand to wipe them off. 
          This task requires <strong>precise hand positioning</strong> and <strong>force control</strong> to ensure the lines are drawn clearly and erased effectively.
        </p>
           
        <div class="allegroupper">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/board_drawing_wiping.mp4" type="video/mp4">
            </video>
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/hardware/new/desk_wiping.mp4" type="video/mp4">
            </video>
        </div>  

        <br>
        <p>
          These results illustrate that our framework maintains compliant contact, 
          adapts effectively to environmental variations, and enables the robot to perform everyday loco-manipulation behaviors. 
          Together, these examples further highlight the <strong>versatility</strong> and <strong>generalization</strong> capability of the Opt2Skill framework across diverse real-world scenarios.
        </p>

      
        <br>
        <br>
        <br>
        <hr class="rounded">

        <h1>Opt2Skill</h1>

        <p>
        Opt2Skill aims to develop an RL-based whole-body controller that enables a humanoid robot to track model-based optimal trajectories. 
        These trajectories contain valuable torque reference that enables a high success rate in multi-contact loco-manipulation tasks. 
        We begin by generating <strong>dynamically feasible</strong> and <strong>task-specific</strong> motions using DDP. 
        These trajectories serve as <strong>high-quality</strong> reference motions that encode contact-rich dynamics, torque limits, and task-solving strategies. 
        We generate a diverse set of such trajectories offline and use them to guide the training of RL policies that directly predict joint-level target positions.
        By leveraging physically consistent references, the policy learns robust control strategies that track diverse reference motions sampled offline and transfer effectively to real hardware.
        </p>

        <div class="approach">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/method_new.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        <h1>Whole-body Reference Motions Generation</h1>

        <p>
            To support robust policy learning across a wide range of loco-manipulation behaviors, 
            we generate reference trajectories for diverse tasks, 
            including <i>walking</i>, <i>jumping</i>, <i>stair climbing</i>, <i>object manipulation</i>, <i>drawing and wiping</i>, and <i>door opening</i>. 
            For each task, we systematically vary key motion parameters such as <i>gait phase</i>, <i>contact mode</i>, <i>walking speed</i>, <i>foot clearance</i>, <i>center of mass position</i>, and <i>target object location</i>. 
            Leveraging the flexibility of our trajectory optimization (TO) framework, 
            we efficiently sample these parameters to produce large datasets of motions that satisfy full-body dynamics, contact constraints, and torque limits.
        <br>
            These reference trajectories include critical <strong>dynamics-relevant properties</strong> such as joint torques and interaction forces, 
            which are often missing from purely <strong>kinematic demonstrations</strong>. 
            Such information can be directly incorporated into policy observations and reward design—facilitating the learning of robust and transferable <strong>contact-rich</strong> loco-manipulation behaviors.
        </p>
        
        
          <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/ref_motion.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <h1>Training in MuJoCo</h1>

        <p>
          We train RL-based motion imitation policies in MuJoCo simulator.
        </p>
        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/training_mujoco.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        <h1 style="font-size:27px;">
        <strong><i>How does the quality and source of motion datasets affect the performance and generalization of Humanoid RL Policies?</i></strong>
        </h1>

        <p>
          We compare Opt2Skill with baselines that rely on human demonstrations and inverse kinematics-based references.
          These results show that <strong>physical feasibility</strong> and <strong>task adaptability</strong> of TO-based references contribute to both <strong>local tracking accuracy</strong> and <strong>long-term global stability</strong>.
        </p>

        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/baselines/table.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <p>
          Opt2Skill exhibits a higher success rate on varying terrains. 
          This highlights the advantages of training with offline-generated, dynamically feasible trajectories compared to fixed human or purely kinematic references.
        </p>
        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/baselines/terrain.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        <h1 style="font-size:27px;">
        <strong><i>What role does torque information play in learning physically consistent and contact-rich humanoid behaviors?</i></strong>
        </h1>

        <p>
          We conduct additional simulation experiments to investigate the effect of torque information in contact-rich tasks. 
          Specifically, we focus on a wiping task that requires controlled contact force between the end effector and a desk surface. 
          We evaluate four ablation baselines:
          <br>

        <strong>Pos</strong>: tracks end-effector positions without any contact or torque information in the observation or reward.</li><br>
        <strong>Pos+F</strong>: adds reference contact force to the observation and includes a contact force tracking reward.</li><br>
        <strong>Pos+T</strong>: adds reference joint torques to the observation and includes a torque tracking reward.</li><br>
        <strong>Pos+F+T</strong>: adds both reference contact force and joint torques in the observation, along with corresponding force and torque tracking rewards.</li>
        </p>
        
        <p>
          We demonstrate that <strong>joint torque information—available only from trajectory optimization (TO)—enhances tracking performance in contact-rich scenarios. 
          Torque information plays a crucial role by guiding <strong><em>when</em></strong> and <strong><em>how</em></strong> force should be applied during motion. 
          Combining both torque and contact force information yields the best tracking performance in loco-manipulation tasks. 
          TO provides reference torques, which help anchor the policy to physically grounded and consistent behaviors.
        </p>
        <p>
          Moreover, <strong>policies trained with TO-generated references adapt when the environment deviates from nominal settings</strong> (e.g., table height). 
          For instance, with a trajectory defined for 0.95 m height and 15 N contact force, 
          the policy maintained wiping at 0.9 m and 1.0 m table heights, 
          achieving average forces of 9.6±1.5 N and 21.3±2.0 N, 
          and tracking errors of 4.1±1.6 cm and 4.5 ± 2.0 cm, respectively.
        </p>

        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/baselines/contactforce.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        <h1>Conclusion</h1>

        <p>
          In this paper, we present a TO-guided RL pipeline for humanoid loco-manipulation. 
          We show that the RL tracking performance is affected by the quality of the motion reference. 
          The full-body-dynamics-based TO provides high-quality and dynamically-feasible trajectories. 
          Based on such trajectories, motion imitation yields better tracking performance, especially through the use of torque information. 
          We demonstrate our sim-to-real results on the humanoid robot Digit with versatile loco-manipulation skills, 
          including dynamic stair traversing, multi-contact box manipulation, and door traversing.
        </p>


        


       
        <h1>BibTeX</h1>
         <p class="bibtex">
            @article{liu2024opt2skill,<br>
            &nbsp;&nbsp;title={Opt2Skill: Imitating Dynamically-feasible Whole-Body Trajectories for Versatile Humanoid Loco-Manipulation},<br>
            &nbsp;&nbsp;author={Liu, Fukang and Gu, Zhaoyuan and Cai, Yilin and Zhou, Ziyi and Jung, Hyunyoung and Jang, Jaehwi and Zhao, Shijie  and Ha, Sehoon and Chen, Yue and Xu, Danfei and Zhao, Ye},<br>
            &nbsp;&nbsp;journal={IEEE Robotics and Automation Letters},<br>
            &nbsp;&nbsp;year={2025},<br>
            &nbsp;&nbsp;publisher={IEEE}<br>
            }
        </p>

        <!-- <h1>Acknowledgements</h1>
        <p>
          The authors would like to thank all the developers of Crocoddyl and Pinocchio for their open source frameworks. 
          We are especially grateful to <a href="https://zhaomingxie.github.io/" target="_blank" style="color: blue;">Zhaoming Xie</a> and <a href="https://cmastalli.github.io/" target="_blank" style="color: blue;">Carlos Mastalli</a> for their professional discussions and insightful feedback.
        </p> -->
       

        <div class="footer">
          <p>This website was developed based on <a href="https://github.com/learning-humanoid-locomotion/learning-humanoid-locomotion.github.io" target="_blank">learning-humanoid-locomotion</a></p>
      </div>
      
    </div>
    <script type="text/javascript">
        /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
        document.querySelector('video').defaultPlaybackRate = 1.0;
        document.querySelector('video').play();

        var videos =document.querySelectorAll('video');
        for (var i=0;i<1;i++)
        {
            videos[i].playbackRate = 1.0;
        }
    </script>
    <script>
        /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
        var videos = document.getElementsByTagName("video");

        function checkScroll() {
            var fraction = 0.5; // Play when 70% of the player is visible.

            for(var i = 0; i < 1; i++) {  // only apply to the first video

                var video = videos[i];

                var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }
        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Function to check if the user is on a mobile device
            function isMobileDevice() {
                return /Mobi|Android/i.test(navigator.userAgent);
            }
            // If the user is on a mobile device, disable autoplay
            if (isMobileDevice()) {
                const videos = document.querySelectorAll('video');
                videos.forEach(video => {
                    video.autoplay = false;
                    video.controls = true;
                });
            }
        });
    </script>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
            type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
            crossorigin="anonymous"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
            type="text/javascript"></script>
  </body>

</html>
